# ğŸ§¹ğŸ¤– Q-Learning & DQN Cleaning Agent â€“ 24CS60R70

Welcome to the **Grid Cleaning Challenge**! This project features a smart little agent ğŸ§  navigating a grid ğŸŸ¦ to clean dirt ğŸ’© using **Q-Learning** and **Deep Q-Networks (DQN)**.

It includes:
- ğŸ“„ Well-documented code
- ğŸ§  Smart heuristics
- ğŸ§ª Evaluation & tuning
- âœ¨ Bonus multi-dirt support!

---

## ğŸ“ Repository Contents

| File | Description |
|------|-------------|
| `24CS60R70.QLearning.py` | ğŸ§  Q-Learning agent (single dirt cell) |
| `24CS60R70_DQN.py` | ğŸ¤– DQN agent using PyTorch |
| `24CS60R70_Bonus.py` | ğŸ§¹ Bonus version with multiple dirts |
| `repoort.pdf` | ğŸ“Š Project report |
| `README.md` | ğŸ“˜ You're reading it! |

---

## âš™ï¸ Requirements

Make sure you have Python â‰¥ 3.6 and install the dependencies:

```bash
pip install numpy matplotlib tqdm joblib gym torch
```

---

## â–¶ï¸ How to Run

### ğŸ§  Q-Learning Agent (Single Dirt)

```bash
python 24CS60R70.QLearning.py --grid_size 10 --episodes 1000
```

ğŸ” **Hyperparameter tuning:**

```bash
python 24CS60R70.QLearning.py --hyperparam
```

ğŸ“ˆ **Evaluation mode:**

```bash
python 24CS60R70.QLearning.py --eval
```

---

### ğŸ¤– DQN Agent (Deep Q-Learning)

```bash
python 24CS60R70_DQN.py --grid_size 10 --episodes 200
```

âš™ï¸ **Tuning for best performance:**

```bash
python 24CS60R70_DQN.py --hyperparam
```

ğŸ§ª **Evaluate across environments:**

```bash
python 24CS60R70_DQN.py --eval
```

ğŸ’¾ **Save your trained model:**

```bash
python 24CS60R70_DQN.py --save my_dqn_model.pth
```

---

### âœ¨ Bonus: Multiple Dirt Cells

```bash
python 24CS60R70_Bonus.py --grid_size 10 --episodes 1000 --num_dirts 3
```

âš™ï¸ **Tune for multiple dirts:**

```bash
python 24CS60R70_Bonus.py --hyperparam
```

ğŸ§ª **Evaluate:**

```bash
python 24CS60R70_Bonus.py --eval
```

---

## ğŸ§¾ Project Report

Check out the detailed report in [`repoort.pdf`](repoort.pdf) ğŸ“‘.  
It includes methodology, experiments, results, and cool visualizations ğŸ“Š.

---

## ğŸ§  Smart Agent Logic

ğŸš€ **Heuristic-based Action Selection:**
- The agent moves toward the **nearest dirt cell** using **Manhattan distance** ğŸ§®.
- If the move is invalid (wall/obstacle), fallback to Q-values or random exploration ğŸŒ€.
- Heuristics applied in both Q-Learning and DQN for smarter behavior!

ğŸ“Œ Modifications are clearly marked with `# ğŸ§  Modified` comments in each script.

---

## ğŸ“‚ File Structure

```
.
â”œâ”€â”€ 24CS60R70.QLearning.py       # ğŸ§  Q-Learning (Single Dirt)
â”œâ”€â”€ 24CS60R70_DQN.py             # ğŸ¤– DQN (Deep Q-Learning)
â”œâ”€â”€ 24CS60R70_Bonus.py           # âœ¨ Q-Learning with Multiple Dirt Cells
â”œâ”€â”€ report.pdf                  # ğŸ“Š Project Report
â””â”€â”€ README.md                    # ğŸ“˜ Interactive Guide
```

---

## ğŸ’¡ Highlights

âœ… Uses OpenAI Gym-style environment  
âœ… Renders grid world with Twemoji CDN-based emoji support ğŸ–¼ï¸  
âœ… Heuristic-driven for realistic movement  
âœ… Supports both single & multiple dirt modes  
âœ… Includes post-training generalization tests across multiple environments

